{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesado de votos\n",
    "\n",
    "Este script modela la rama real-time (es decir, la única rama que hay) de la aplicación. Spark nos genera un dataframe continuo sobre el que podemos aplicar filtros y expresiones como en un dataframe normal, con la salvedad de que el resultado no se devuelve en el mismo instante de ejecutar un `df.show()` o `df.write()`, sino que debemos decidir en el modo de escritura con `.outputMode()`. Además, no todos los formatos de escritura soportan todos los modos de salida: https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#basic-concepts y https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#output-modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "PACKAGES = \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredVotes\") \\\n",
    "    .config(\"spark.jars.packages\", PACKAGES)\\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ejercicios.votes import TOPIC_VOTES, TOPIC_VOTES_ENRICHED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos el dataframe de manera similar a los dataframes de orígenes estáticos: indicamos el origen, Kafka, y varios parámetros de configuración: la dirección del broker, el topic y el offset inicial: https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#creating-a-kafka-source-for-batch-queries. Debemos subscribirnos desde el offset más antiguo para poder recalcular el resultado sobre todos los votos recibidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"subscribe\", TOPIC_VOTES_ENRICHED) \\\n",
    "  .load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los mensajes llegan en formato JSON, pero al contrario que con `spark.read.csv`, debemos indicar el esquema completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"CODIGO\", IntegerType()),\n",
    "    StructField(\"COMUNIDAD\", StringType()),\n",
    "    StructField(\"PROVINCIA\", StringType()),\n",
    "    StructField(\"MUNICIPIO\", StringType()),\n",
    "    StructField(\"PARTIDO\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar el modo de salida por consola para hacer debugging. Si usamos el modo de salida `append`, en cada microbatch sólo se imprimen los mensajes procesados en ese batch. For formato por consola no soporta modo `complete`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df \\\n",
    "    .selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING) AS value\") \\\n",
    "    .withColumn(\"value_json\", fn.from_json(col('value'), schema)) \\\n",
    "    .select('topic', 'value_json') \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al contrario que los clientes `Consumer` de otros ejercicios, `query.start()` arranca la query en el cluster de Spark sin bloquear el proceso python. Podemos pararla expresamente con `query.stop()` o bloquear el proceso hasta que termine la query con `query.awaitTermination()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos volcar los datos en una vista SQL en memoria. Esta vista se puede leer como un dataframe por otros procesos de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df \\\n",
    "    .selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING) AS value\") \\\n",
    "    .withColumn(\"value_json\", fn.from_json(col('value'), schema)) \\\n",
    "    .select('topic', 'value_json') \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName('preview') \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|         topic|          value_json|\n",
      "+--------------+--------------------+\n",
      "|VOTES_ENRICHED|[31002, Comunidad...|\n",
      "|VOTES_ENRICHED|[20029, País Vasc...|\n",
      "|VOTES_ENRICHED|[17048, Catalunya...|\n",
      "|VOTES_ENRICHED|[40041, Castilla ...|\n",
      "|VOTES_ENRICHED|[45186, Castilla ...|\n",
      "|VOTES_ENRICHED|[16264, Castilla ...|\n",
      "|VOTES_ENRICHED|[43141, Catalunya...|\n",
      "|VOTES_ENRICHED|[25155, Catalunya...|\n",
      "|VOTES_ENRICHED|[46096, Comunitat...|\n",
      "|VOTES_ENRICHED|[17187, Catalunya...|\n",
      "|VOTES_ENRICHED|[9198, Castilla y...|\n",
      "|VOTES_ENRICHED|[17155, Catalunya...|\n",
      "|VOTES_ENRICHED|[7023, Illes Bale...|\n",
      "|VOTES_ENRICHED|[9328, Castilla y...|\n",
      "|VOTES_ENRICHED|[9148, Castilla y...|\n",
      "|VOTES_ENRICHED|[25131, Catalunya...|\n",
      "|VOTES_ENRICHED|[45152, Castilla ...|\n",
      "|VOTES_ENRICHED|[29076, Andalucía...|\n",
      "|VOTES_ENRICHED|[37035, Castilla ...|\n",
      "|VOTES_ENRICHED|[20056, País Vasc...|\n",
      "+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM preview').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tabla de destino y su dataframe asociada tiene un esquema, al igual que cualquier otro dataframe. La función `to_json` genera una columna de tipo complejo que podemos simplificar con una operación select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- topic: string (nullable = true)\n",
      " |-- value_json: struct (nullable = true)\n",
      " |    |-- CODIGO: integer (nullable = true)\n",
      " |    |-- COMUNIDAD: string (nullable = true)\n",
      " |    |-- PROVINCIA: string (nullable = true)\n",
      " |    |-- MUNICIPIO: string (nullable = true)\n",
      " |    |-- PARTIDO: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM preview').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df \\\n",
    "    .selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING) AS value\") \\\n",
    "    .withColumn(\"value_json\", fn.from_json(col('value'), schema)) \\\n",
    "    .select('value_json.COMUNIDAD', 'value_json.PROVINCIA', 'value_json.PARTIDO') \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName('preview2') \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+\n",
      "|           COMUNIDAD|         PROVINCIA|             PARTIDO|\n",
      "+--------------------+------------------+--------------------+\n",
      "|     Castilla y León|            Burgos|                 VOX|\n",
      "|           Andalucía|           Granada|               Nulos|\n",
      "|           Catalunya|         Tarragona|RECORTES_CERO_GRU...|\n",
      "|       Illes Balears|     Illes Balears|                  PP|\n",
      "|     Castilla y León|         Salamanca|                  Cs|\n",
      "|           Andalucía|            Málaga|                 AND|\n",
      "|Comunitat Valenciana|Alicante / Alacant|                 VOX|\n",
      "+--------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM preview2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras leer el mensaje, una de las tareas es validar la _firma del mensaje_. El enunciado nos indica que asumamos que la firma digital está incluida en el mensaje y podemos hacer uso de una función que devuelva un booleano si la firma es satisfactoria. Como ya sabemos usar [UDFs](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.udf), podemos escribir la función en python y convertirla a UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_signature(comunidad, provincia, municipio):\n",
    "    return 'OK'\n",
    "\n",
    "udf_process_signature = fn.udf(process_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya estamos en disposición de agregar los resultados por comunidad autónoma y provincia. Al ejecutar una agregación debemos cambiar al modo de salida `complete` o `update`. El formato `memory` no soporta `update`, así que actualizaremos la tabla completa en cada iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df \\\n",
    "    .selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING) AS value\") \\\n",
    "    .withColumn(\"value_json\", fn.from_json(col('value'), schema)) \\\n",
    "    .select('value_json.COMUNIDAD', 'value_json.PROVINCIA', 'value_json.MUNICIPIO', 'value_json.PARTIDO') \\\n",
    "    .withColumn('SIGNATURE', udf_process_signature(col('COMUNIDAD'), col('PROVINCIA'), col('MUNICIPIO'))) \\\n",
    "    .where(~ fn.isnull(col('SIGNATURE'))) \\\n",
    "    .groupBy('COMUNIDAD', 'PROVINCIA', 'PARTIDO') \\\n",
    "    .agg(fn.count('*').alias('VOTOS')) \\\n",
    "    .sort(col('COMUNIDAD').asc(), col('PROVINCIA').asc(), col('VOTOS').desc()) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName('dashboard') \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados\n",
    "\n",
    "Sobre esta tabla podemos aplicar más filtros y podríamos usarla como origen de datos para escribir en un fichero externo (por ejemplo, al terminar las votaciones) o para mostrar gráficos con la composición de escaños."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+-----+\n",
      "|COMUNIDAD|PARTIDO                  |VOTOS|\n",
      "+---------+-------------------------+-----+\n",
      "|Andalucía|Nulos                    |42   |\n",
      "|Andalucía|Cs                       |37   |\n",
      "|Andalucía|PSOE                     |34   |\n",
      "|Andalucía|PODEMOS_IU_EQUO          |28   |\n",
      "|Andalucía|PACMA                    |28   |\n",
      "|Andalucía|Blanco                   |28   |\n",
      "|Andalucía|PP                       |24   |\n",
      "|Andalucía|RECORTES_CERO_GRUPO_VERDE|23   |\n",
      "|Andalucía|VOX                      |18   |\n",
      "|Andalucía|PCPE                     |17   |\n",
      "|Andalucía|UPyD                     |12   |\n",
      "|Andalucía|FE_de_las_JONS           |10   |\n",
      "|Andalucía|AND                      |8    |\n",
      "|Andalucía|EB                       |4    |\n",
      "|Andalucía|IZAR                     |4    |\n",
      "|Andalucía|CILUS                    |3    |\n",
      "|Andalucía|PCOE                     |3    |\n",
      "+---------+-------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COMUNIDAD, PARTIDO, sum(VOTOS) as VOTOS\n",
    "  FROM dashboard\n",
    "  WHERE VOTOS > 2 and COMUNIDAD LIKE 'And%'\n",
    "  GROUP BY COMUNIDAD, PARTIDO\n",
    "  ORDER BY VOTOS DESC\n",
    "\"\"\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura Kappa: cambio de core\n",
    "\n",
    "El enunciado nos indica que a mitad de jornada se detecta que en una de las provincias hay problemas técnicos y la firma de los votos puede haber sido manipulada. Debemos recalcular los votos sin detener el sistema de recepción, ya que el resto de mesas electorales deben seguir funcionando.\n",
    "\n",
    "En este caso podemos calcular una segunda tabla en la que descartamos todos los votos de la provincia afectada. Sólo es necesario cambiar la lógica de la función que valida las firmas. En un caso real reescribiríamos el código y arrancaríamos un segundo proceso (o contenedor, o llamada a pyspark-submit). En este caso podemos ejecutar la query desde el mismo notebook por simplificar.\n",
    "\n",
    "Si la provincia afectada es, por ejemplo, Granada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_signature_v2(comunidad, provincia, municipio):\n",
    "    if provincia == 'Granada':\n",
    "        return None\n",
    "    else:\n",
    "        return 'OK'\n",
    "\n",
    "udf_process_signature_v2 = fn.udf(process_signature_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = df \\\n",
    "    .selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING) AS value\") \\\n",
    "    .withColumn(\"value_json\", fn.from_json(col('value'), schema)) \\\n",
    "    .select('value_json.COMUNIDAD', 'value_json.PROVINCIA', 'value_json.MUNICIPIO', 'value_json.PARTIDO') \\\n",
    "    .withColumn('SIGNATURE', udf_process_signature_v2(col('COMUNIDAD'), col('PROVINCIA'), col('MUNICIPIO'))) \\\n",
    "    .where(~ fn.isnull(col('SIGNATURE'))) \\\n",
    "    .groupBy('COMUNIDAD', 'PROVINCIA', 'PARTIDO') \\\n",
    "    .agg(fn.count('*').alias('VOTOS')) \\\n",
    "    .sort(col('COMUNIDAD').asc(), col('PROVINCIA').asc(), col('VOTOS').desc()) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName('dashboard_v2') \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+-----+\n",
      "|COMUNIDAD|PARTIDO                  |VOTOS|\n",
      "+---------+-------------------------+-----+\n",
      "|Andalucía|Cs                       |36   |\n",
      "|Andalucía|Nulos                    |34   |\n",
      "|Andalucía|PSOE                     |25   |\n",
      "|Andalucía|PACMA                    |22   |\n",
      "|Andalucía|PODEMOS_IU_EQUO          |21   |\n",
      "|Andalucía|PCPE                     |21   |\n",
      "|Andalucía|Blanco                   |18   |\n",
      "|Andalucía|RECORTES_CERO_GRUPO_VERDE|17   |\n",
      "|Andalucía|PP                       |16   |\n",
      "|Andalucía|VOX                      |12   |\n",
      "|Andalucía|FE_de_las_JONS           |10   |\n",
      "|Andalucía|UPyD                     |9    |\n",
      "|Andalucía|AND                      |8    |\n",
      "|Andalucía|EB                       |4    |\n",
      "|Andalucía|CILUS                    |3    |\n",
      "|Andalucía|PCOE                     |3    |\n",
      "+---------+-------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COMUNIDAD, PARTIDO, sum(VOTOS) as VOTOS\n",
    "  FROM dashboard_v2\n",
    "  WHERE VOTOS > 2 and COMUNIDAD LIKE 'And%'\n",
    "  GROUP BY COMUNIDAD, PARTIDO\n",
    "  ORDER BY VOTOS DESC\n",
    "\"\"\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
